<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <title>Test LLM Integration</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }

        .test-section {
            background: #f5f5f5;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            border-left: 4px solid #007bff;
        }

        .success {
            border-left-color: #28a745;
        }

        .warning {
            border-left-color: #ffc107;
        }

        .error {
            border-left-color: #dc3545;
        }

        .llm-config-section {
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 20px;
            margin-top: 15px;
        }

        .form-group {
            margin: 10px 0;
        }

        .checkbox-label {
            display: flex;
            align-items: center;
            gap: 6px;
            font-size: 13px;
            cursor: pointer;
        }

        .checkbox-label input[type="checkbox"] {
            width: auto;
            margin: 0;
        }

        input,
        select,
        button {
            padding: 8px 12px;
            margin: 5px;
            border: 1px solid #ccc;
            border-radius: 4px;
        }

        button {
            background: #007bff;
            color: white;
            cursor: pointer;
        }

        button:hover {
            background: #0056b3;
        }

        .log {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 4px;
            padding: 10px;
            margin: 10px 0;
            font-family: monospace;
            font-size: 12px;
            max-height: 300px;
            overflow-y: auto;
        }

        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            font-family: 'Monaco', 'Menlo', 'Consolas', monospace;
            font-size: 12px;
            margin: 10px 0;
        }

        .endpoint-examples {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin: 15px 0;
        }

        .endpoint-card {
            background: white;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            padding: 15px;
        }

        .endpoint-card h4 {
            margin: 0 0 10px 0;
            color: #495057;
        }

        .endpoint-url {
            font-family: monospace;
            background: #f8f9fa;
            padding: 5px 8px;
            border-radius: 4px;
            font-size: 11px;
            word-break: break-all;
        }
    </style>
</head>

<body>
    <h1>üß™ Test: LLM Integration for Field Evaluation</h1>

    <div class="test-section success">
        <h3>‚úÖ Implementation Complete</h3>
        <p><strong>What was implemented:</strong></p>
        <ul>
            <li>‚úÖ Created comprehensive LLMService.js with OpenAI-compatible API support</li>
            <li>‚úÖ Added LLM configuration UI in popup with toggle between webhook/LLM modes</li>
            <li>‚úÖ <strong>NEW:</strong> Default to OpenRouter for unified model access</li>
            <li>‚úÖ <strong>NEW:</strong> Enhanced model selection with grouped presets + custom model option</li>
            <li>‚úÖ Updated all services (background, message, capture) to support LLM integration</li>
            <li>‚úÖ Maintained all existing functionality while adding LLM alternative</li>
            <li>‚úÖ Added proper error handling, cancellation, and event tracking for LLM requests</li>
            <li>‚úÖ Supports custom system prompts with field criteria injection</li>
            <li>‚úÖ Works with page refresh and capture delay features</li>
            <li>‚úÖ <strong>NEW:</strong> Fixed storage mismatch that prevented fields from being sent</li>
        </ul>

        <p><strong>üÜï Latest Enhancements:</strong></p>
        <ul>
            <li><strong>OpenRouter Default:</strong> Pre-configured with OpenRouter API for easy model access</li>
            <li><strong>Model Presets:</strong> Organized dropdown with OpenRouter, Direct API, and Local model options
            </li>
            <li><strong>Custom Models:</strong> "Other/Custom Model" option for manual model name entry</li>
            <li><strong>Better UX:</strong> Grouped model options and improved visual hierarchy</li>
            <li><strong>Smart Defaults:</strong> OpenRouter + GPT-4o selected by default for optimal experience</li>
        </ul>
    </div>

    <div class="test-section">
        <h3>üîß LLM Configuration Demo</h3>
        <p>This simulates the new LLM configuration interface in the popup:</p>

        <div class="form-group">
            <label class="checkbox-label">
                <input type="checkbox" id="demo-llm-toggle">
                <span>Use LLM Analysis instead of webhook</span>
            </label>
        </div>

        <div id="demo-llm-config" class="llm-config-section" style="display: none;">
            <div class="form-group">
                <label for="demo-api-url">LLM API URL:</label>
                <input type="url" id="demo-api-url" placeholder="https://openrouter.ai/api/v1/chat/completions"
                    style="width: 100%; max-width: 400px;" value="https://openrouter.ai/api/v1/chat/completions" />
            </div>

            <div class="form-group">
                <label for="demo-api-key">API Key:</label>
                <input type="password" id="demo-api-key" placeholder="sk-or-..."
                    style="width: 100%; max-width: 400px;" />
            </div>

            <div class="form-group">
                <label for="demo-model">Model:</label>
                <select id="demo-model" style="width: 100%; max-width: 400px;">
                    <optgroup label="OpenRouter Models">
                        <option value="openai/gpt-4o" selected>OpenAI GPT-4o</option>
                        <option value="openai/gpt-4o-mini">OpenAI GPT-4o Mini</option>
                        <option value="anthropic/claude-3.5-sonnet">Anthropic Claude 3.5 Sonnet</option>
                        <option value="google/gemini-pro-vision">Google Gemini Pro Vision</option>
                        <option value="meta-llama/llama-3.2-90b-vision-instruct">Llama 3.2 90B Vision</option>
                    </optgroup>
                    <optgroup label="Direct API Models">
                        <option value="gpt-4o">GPT-4o (Direct OpenAI)</option>
                        <option value="claude-3-5-sonnet-20241022">Claude 3.5 Sonnet (Direct Anthropic)</option>
                    </optgroup>
                    <option value="custom">Other/Custom Model...</option>
                </select>
            </div>

            <div class="form-group" id="demo-custom-model-group"
                style="display: none; background: #f1f3f4; border: 1px dashed #ccc; padding: 10px; border-radius: 4px; margin-top: 8px;">
                <label for="demo-custom-model">Custom Model Name:</label>
                <input type="text" id="demo-custom-model" placeholder="e.g., your-custom-model-name"
                    style="width: 100%; max-width: 400px; font-family: monospace;" />
            </div>

            <div class="form-group">
                <label for="demo-temperature">Temperature:</label>
                <input type="number" id="demo-temperature" min="0" max="1" step="0.1" value="0.1" />

                <label for="demo-max-tokens" style="margin-left: 20px;">Max Tokens:</label>
                <input type="number" id="demo-max-tokens" min="100" max="4000" step="100" value="1000" />
            </div>
        </div>

        <button onclick="testLlmConfig()">üß™ Test Configuration</button>
        <div id="config-results" class="log"></div>
    </div>

    <div class="test-section">
        <h3>üéØ System Prompt Template</h3>
        <p>The LLM receives this system prompt with field criteria dynamically injected:</p>

        <div class="code-block">Your job is very important and the results you provide will serve as a gatekeeper for
            actions taken in an automated system. You will behave as a highly accurate frame-by-frame screenshot
            processing engine, where you will be passed an image and a set of one or more fields accompanied by a value
            for each which is the key criteria necessary to evaluate for a boolean true or false value according to your
            image analysis.

            You will respond in pure parseable JSON string from start to finish without any markdown containing all
            original field(s), each accompanied by an array of the resulting boolean, and resulting probability
            represented by a floating point number between 0 and 1 that your analysis resulted in a boolean that is 100%
            correct according to the screenshot. Then at the end of the json response, you will append an additional
            field named "reason" which will contain a brief explanation of what you saw in the image and the state of
            the screenshot.

            Here are the fields and their criteria for evaluation:
            {{ FIELDS_JSON }}</div>

        <p><strong>Expected LLM Response Format:</strong></p>
        <div class="code-block">{
            "login_button_visible": [true, 0.95],
            "error_message_shown": [false, 0.88],
            "user_logged_in": [false, 0.92],
            "reason": "I can see a login form with username and password fields clearly visible. There's a blue 'Sign
            In' button at the bottom. No error messages are displayed and the user appears to be on the login page, not
            logged in yet."
            }</div>
    </div>

    <div class="test-section">
        <h3>üåê Supported API Endpoints</h3>
        <p>The LLMService now defaults to <strong>OpenRouter</strong> for easy access to multiple models:</p>

        <div class="endpoint-examples">
            <div class="endpoint-card" style="border: 2px solid #28a745;">
                <h4>üéØ OpenRouter (Default)</h4>
                <div class="endpoint-url">https://openrouter.ai/api/v1/chat/completions</div>
                <p><strong>Recommended!</strong> Single API for GPT-4, Claude, Gemini, Llama models. Competitive pricing
                    with failover.</p>
                <p><strong>Models:</strong> openai/gpt-4o, anthropic/claude-3.5-sonnet, google/gemini-pro-vision,
                    meta-llama/llama-3.2-90b-vision-instruct</p>
            </div>

            <div class="endpoint-card">
                <h4>OpenAI Direct</h4>
                <div class="endpoint-url">https://api.openai.com/v1/chat/completions</div>
                <p>Direct OpenAI API access. Requires OpenAI API key.</p>
                <p><strong>Models:</strong> gpt-4o, gpt-4o-mini, gpt-4-vision-preview</p>
            </div>

            <div class="endpoint-card">
                <h4>Anthropic Direct</h4>
                <div class="endpoint-url">https://api.anthropic.com/v1/messages</div>
                <p>Direct Anthropic API access. Requires Anthropic API key.</p>
                <p><strong>Models:</strong> claude-3-5-sonnet-20241022, claude-3-opus-20240229</p>
            </div>

            <div class="endpoint-card">
                <h4>Local/Self-hosted</h4>
                <div class="endpoint-url">http://localhost:11434/v1/chat/completions</div>
                <p>Works with Ollama, LM Studio, vLLM, or any OpenAI-compatible server.</p>
                <p><strong>Models:</strong> llava, moondream, cogvlm, or any custom model name</p>
            </div>
        </div>

        <p><strong>üéÅ Why OpenRouter?</strong></p>
        <ul>
            <li><strong>One API Key:</strong> Access to GPT-4, Claude, Gemini, and 100+ models</li>
            <li><strong>Competitive Pricing:</strong> Often cheaper than direct APIs</li>
            <li><strong>Automatic Failover:</strong> If one model is down, tries alternatives</li>
            <li><strong>Usage Analytics:</strong> Track costs and usage across models</li>
            <li><strong>No Vendor Lock-in:</strong> Easy to switch between models</li>
        </ul>
    </div>

    <div class="test-section">
        <h3>üîÑ Request/Response Flow</h3>
        <p><strong>LLM Request Process:</strong></p>
        <ol>
            <li>User configures LLM API URL, key, and model in popup</li>
            <li>User toggles "Use LLM Analysis instead of webhook"</li>
            <li>User clicks "Capture Screenshot Now (LLM Analysis)"</li>
            <li>System captures screenshot and converts to base64</li>
            <li>System builds OpenAI-style request with system prompt + image</li>
            <li>LLM analyzes image according to field criteria</li>
            <li>System parses JSON response and updates field status</li>
            <li>Field webhooks fire based on results (if configured)</li>
            <li>Event is recorded in history with full details</li>
        </ol>

        <button onclick="testRequestFlow()">üß™ Test Request Flow</button>
        <div id="flow-results" class="log"></div>
    </div>

    <div class="test-section">
        <h3>‚ö° Key Advantages</h3>
        <ul>
            <li><strong>No n8n Required:</strong> Direct LLM integration eliminates need for workflow platform</li>
            <li><strong>Multimodal Analysis:</strong> AI can understand image content, not just extract text</li>
            <li><strong>Flexible Models:</strong> Support for GPT-4 Vision, Claude 3.5 Sonnet, and local models</li>
            <li><strong>Same Field System:</strong> Reuses existing field configuration and webhook triggers</li>
            <li><strong>Better Accuracy:</strong> AI can understand visual context, layout, and complex conditions</li>
            <li><strong>Cost Effective:</strong> Pay per API call instead of maintaining workflow infrastructure</li>
            <li><strong>Privacy Options:</strong> Can use local/self-hosted models for sensitive content</li>
            <li><strong>Instant Setup:</strong> Just API key and URL, no workflow configuration needed</li>
        </ul>
    </div>

    <div id="overall-log" class="log"></div>

    <script>
        function log(message, elementId = 'overall-log') {
            const logElement = document.getElementById(elementId);
            const timestamp = new Date().toLocaleTimeString();
            logElement.innerHTML += `[${timestamp}] ${message}\n`;
            logElement.scrollTop = logElement.scrollHeight;
        }

        // Demo LLM toggle functionality
        document.getElementById('demo-llm-toggle').addEventListener('change', function () {
            const configSection = document.getElementById('demo-llm-config');
            if (this.checked) {
                configSection.style.display = 'block';
                log('‚úÖ LLM mode enabled - configuration panel shown');
            } else {
                configSection.style.display = 'none';
                log('‚úÖ Webhook mode enabled - configuration panel hidden');
            }
        });

        // Demo model selection functionality
        document.getElementById('demo-model').addEventListener('change', function () {
            const customModelGroup = document.getElementById('demo-custom-model-group');
            const customModelInput = document.getElementById('demo-custom-model');

            if (this.value === 'custom') {
                customModelGroup.style.display = 'block';
                log('‚úÖ Custom model option selected - custom input shown');
                log('üí° Enter your custom model name (e.g., "my-local-model")');
            } else {
                customModelGroup.style.display = 'none';
                customModelInput.value = '';
                log(`‚úÖ Selected preset model: ${this.value}`);
            }
        });

        document.getElementById('demo-custom-model').addEventListener('input', function () {
            if (this.value.trim()) {
                log(`‚úÖ Custom model set to: ${this.value.trim()}`);
            }
        });

        function testLlmConfig() {
            log('üß™ Testing LLM configuration...', 'config-results');

            const apiUrl = document.getElementById('demo-api-url').value;
            const apiKey = document.getElementById('demo-api-key').value;
            const model = document.getElementById('demo-model').value;
            const customModel = document.getElementById('demo-custom-model').value;
            const temperature = document.getElementById('demo-temperature').value;
            const maxTokens = document.getElementById('demo-max-tokens').value;

            // Determine final model name
            const finalModel = model === 'custom' && customModel.trim() ? customModel.trim() : model;

            const config = {
                apiUrl: apiUrl || 'https://openrouter.ai/api/v1/chat/completions',
                apiKey: apiKey || 'sk-or-example-key',
                model: finalModel,
                temperature: parseFloat(temperature),
                maxTokens: parseInt(maxTokens)
            };

            if (model === 'custom') {
                config.customModel = customModel.trim();
                log('üìù Custom Model Configuration:', 'config-results');
            } else {
                log('üìù Preset Model Configuration:', 'config-results');
            }

            log(JSON.stringify(config, null, 2), 'config-results');
            log('‚úÖ Configuration would be saved per domain', 'config-results');

            if (model === 'custom' && !customModel.trim()) {
                log('‚ö†Ô∏è Custom model selected but no model name entered', 'config-results');
            } else {
                log('‚úÖ Ready for LLM analysis!', 'config-results');
            }
        }

        function testRequestFlow() {
            log('üß™ Testing LLM request flow...', 'flow-results');

            // Simulate the complete flow
            const mockFields = [
                { name: 'login_button', criteria: 'Check if a login or sign-in button is visible' },
                { name: 'error_message', criteria: 'Look for any error messages or warnings displayed' }
            ];

            log('1. üì∏ Screenshot captured and converted to base64', 'flow-results');
            log('2. üîß Building LLM request with system prompt...', 'flow-results');

            const systemPrompt = `Your job is very important and the results you provide will serve as a gatekeeper for actions taken in an automated system...

Here are the fields and their criteria for evaluation:
${JSON.stringify(mockFields, null, 2)}`;

            log('3. üì§ Sending to LLM API...', 'flow-results');

            // Mock LLM response
            const mockResponse = {
                login_button: [true, 0.95],
                error_message: [false, 0.88],
                reason: "I can see a clear login form with a blue 'Sign In' button visible. No error messages are currently displayed on the page."
            };

            log('4. ‚úÖ LLM response received:', 'flow-results');
            log(JSON.stringify(mockResponse, null, 2), 'flow-results');
            log('5. üîÑ Field status updated in UI', 'flow-results');
            log('6. üìû Field webhooks triggered for TRUE results', 'flow-results');
            log('7. üìù Event recorded in history', 'flow-results');
            log('‚úÖ Complete LLM analysis flow successful!', 'flow-results');
        }

        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            log('üöÄ LLM Integration test page loaded');
            log('üéØ Ready to test LLM field evaluation functionality');
            log('üí° This replaces n8n webhooks with direct AI analysis');
            log('üîß Toggle LLM mode above to see configuration options');
        });
    </script>
</body>

</html>